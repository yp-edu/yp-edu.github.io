---
title: My Approach to AI Safety
tldr: Shortcoming issues should not be undermined because, if not tackled immediately, they will make Alignment a lot more complex. I am convinced that interpretability will be the best tool for monitoring and control, and for that, I will pursue this agenda through research and entrepreneurship.
tags:
  - AIS
  - Agenda
  - Startup
references: 
aliases: 
crossposts: 
publishedOn: 2023-11-02
editedOn: 2023-12-14
authors:
  - "[[Yoann Poupart]]"
readingTime: 3
---
> [!caution] WIP
> 
> This article is a work in progress.

> [!tldr] TL;DR
> 
> Shortcoming issues should not be undermined because, if not tackled immediately, they will make Alignment a lot more complex. I am convinced that interpretability will be the best tool for monitoring and control, and for that, I will pursue this agenda through research and entrepreneurship.

> [!example] Table of content
> 
> - [My Claims](#my-claims)
> - [Short-term vs Long-term](#short-term-vs-long-term)
> 	- [The Industry Pressure](#the-industry-pressure)
> 	- [The Curse of AI Doomerism](#the-curse-of-ai-doomerism)
> 	- [Viable Paths](#viable-paths)
> - [Entrepreneurship](#entrepreneurship)
> 	- [Precious Mindset](#precious-mindset)
> 	- [Founding an AI Startup](#founding-an-ai-startup)
> 	- [Founding an Org](#founding-an-org)
> - [My General Approach](#my-general-approach)
> 	- [My AIS Edges](#my-ais-edges)
> 	- [Brief Agenda](#brief-agenda)
> 	- [What's next?](#whats-next)

## My Claims

- It is necessary to pursue research for short-term and long-term AIS in the meantime 
- Entrepreneurship can be highly valuable to contribute to AIS
- My impact is likely to be net positive for Alignment

> [!caution] Epistemic Status
> 
> I am early in my career and even more in AIS; while I stand by the words below, I might not have considered the whole picture, and I don't rule out the possibility of changing my mind.

## Short-term vs Long-term

### The Industry Pressure

When thinking about what could go wrong with AI it often comes to competitiveness and arm race dynamics. OpenAI, DeepMind & co are often critisize as having the monopole on AI and are urged to make their research more open. Yet I really don't see where letting all the models open-source could solve such an issue. Open-source doesn't genuinely reduce risks and can definitely make competition harder and faster. I am much more comfortable with the [Frontier Model Forum](https://openai.com/blog/frontier-model-forum) to foster cooperation and ensure that the biggest players don't fall into a race.

Regarding sustainability, industry has a vile tendency towards technology solutionism, relying too heavily on technological solutions without fully considering other leverage. Each capability increase has an associated environmental or cyber-security cost. And such rapid increase is not controllable nor sustainable in the long run. Regulating early might be the most efficient but also the only effective way to gain security as fast as the capabilities improve. AI governance is definitely needed as the transforming potential of AI is first and foremost a societal issue.

From a safety point of view regulation might also have some gotchas. The two main drawbacks to take into account are:
- Control loss:
With regulation might come a slow down on technological advancements, thus country unaffected by regulation might take an upper hand. The key point here is to devise regulation that support innovation e.g. highly regulating the use cases but less the technology.
- Goodharting:
As it has been seen with model evaluation and could also be the case with model interpretability. Companies will try to beat the benchmark or improve some interpretability metrics which doesn't necessarily correlate with safety. It is therefore primordial to advance technically advance these safeguards as much as possible.

### The Curse of AI Doomerism

I want to address a mitigated critics against AI Doomerism which is quite represented in the AIS sphere. As an optimism by nature I might be failing to see the same future possibilities and my arguments could be inaudible. In my Doomerism definition I englobe focusing on long-term safety issues firstly because of existential risks.

The first critics is the tendency to only consider worst-case scenario. While I value some aspects of this approach it often lead to possibilism workin on highly theoretical and abstract subjects with lesser impact on immediate issues. Solving the worst-case scenario will not solve less dangerous ones.

> [!danger] Gotcha
> 
> Alignment is obviously a double-edged sword. If you can align an AGI to be benevolent, you'll also be able to align it to be rogue. Misuse cannot be solved by Alignment alone.

It is crucial to address short-term issues that could potentially destroy Alignment research if they are not dealt with rapidly. It is essential to keep in mind that there are also other pressing matters that require attention, such as climate change, cybersecurity, and war. I value an prioritization based on danger weighted by occurrence probability and timeline. 
 
I finally have critics on some speech I had the occasion to listen which is by far a loud minority. The righteous and moralism speeches which I doesn't foster debate and isolate the AIS community. This is dramatic when collaboration outside of the AIS sphere might just accelerate safety. Then I sometimes feel too many people harboring sentiments of culpability and pessimism. While skepticism is a really important researcher skill I think that pessimism is bad and ultimately prevent from long range expected rewards. It can also repulse people and grow isolation.

### Viable Paths

First I feel that reduction approaches like symbolism conversion are a great way to make models interpretable by default but it won't scale. Emergent properties are really hard to be made interpretable by default and this path might require too much time compared to the capabilities development.

 One approach is to focus on closed evaluations, which involve testing models on a limited set of data to gain insights into their behaviour. Another option is to conduct both closed and open research into model interpretability, which can help to identify patterns and trends that might not be immediately apparent in closed evaluations. 
 
 Finally, some researchers are working to mathematically ground interpretability in order to develop a theory of control that can help to guide the development of more interpretable models in the future.

## Entrepreneurship

### Precious Mindset

One of the main points of disagreement I have with Yudowsky and others is regarding the driving forces behind the development of AIS. Safety cannot emerge in domain where there is no market nor innovation. Safety is a logical response to ongoing concerns and needs to be part of the system to truly model it. I firmly believe that creation, innovation, and people focus should be the key drivers for the upcoming new AI era.

In order to ensure a significant impact and bring about change in the industry and, more generally, in the system, it is essential to be an active participant rather than just a pawn. Easier said than done! Yet, I believe than entrepreneurship gives the best leverage for this, whereas employees goals are crushed by hierarchy and industry pressure and academics dreams of impact can fade behind the publication pressure.

### Founding an AI Startup

Founding an startup can be an extremely valuable experience, as it allows for the development of a diverse skill set. It also provides an opportunity to gain a deeper understanding of the market and industry responses. By starting a company, you can work towards getting acknowledgement and influence in the industry, which can be invaluable for future success.

Obviously AI will be the most transformative technology of our era and there is plenty of possibilities to make AI application into startups. I also think that this could be a first step towards creating a safety oriented startup which can be slightly different as described in the next section [Founding an Org](#founding-an-org).

> [!tip] Ulterior Motive
> 
> The people need to be empowered by AI for their own good.

Although I am still at the beginning of this journey I have learned a lot. It motivates me deeply and let my creative and perfectionist selves to be wholly expressed. I'll be outlining this in a future post "Creating a Startup".

### Founding an Org

Founding an organization could be one of the most effective ways to contribute to the field of AI safety. By establishing an organization, individuals or groups can achieve independence for their research. it obviously is dependent on the board chosen oversee its operations. 

Moreover, there is a growing need for bridges between the industry and AI research, and an organization can play a vital role in filling this gap. It operates somewhat similarly than classical startup regarding funding and product drive

## My General Approach

### My AIS Edges ###

In order to maximize my impact on improving safety in AI it is important that I take advantage of all my edges. My first edge is technical problem-solving taking advantage of my skills ranging form ML engineering to theoretical fundamentals from mathematics and physics. This enable me tackle concrete AI Safety problem not being afraid of mathematical aspect while also being able to iterate quickly with simple concrete experiments. Obviously my scope is limited and I cannot really have a great impact on too theoretical, philosophical or political aspects for now. 

Then my second relevant edge for AIS lies in teaching and mentoring which comes from my experiences and personal fit. I'm particularly passionate about mentoring and I have helped students in private lessons through all my study years in mathematics, CS, AI and more. I have been an high school teacher for a few months introducing the fundamentals of CS. I am curious and perfectionist by nature (fundamental teaching skills IMO) and I think explaining concepts in the simplest way has always helped me. I would love to contribute to AIS by helping out with any upcoming AIS training programs, and overall, my goal is to continue learning and growing along with the field of AIS.

I left my entrepreneur edge apart since I am only beginning to grow and explore this facet. What is more this edge is rather catalyst and could serve the two previous ones outlined.

### Brief Agenda

My agenda is focused on interpretability as it accors to me to be the only possible way to scale in the long term keeping up with capabilities increase. I am particularly enthusiast with grounding interpretability in theory whenever it's possible and relevant.

I also value model evaluation, even if it might be harder to scale in the long-term, this approach is for now the effective and promising on the short-term. In particular I want to explore the best way on how evaluation and interpretability could be intertwined.

### What's next?

I am currently undergoing a research internship on interpretability and I maintain my participation in different AIS program in parallel. I am finishing the program [SPAR](https://berkeleyaisafety.com/spar) and will be involved in a research project with [Apart Research](https://apartresearch.com/lab).

I am also committed to developing my startup with my co-founders. While our focus is on empowering small shopkeepers with our solutions I am also focusing on bringing the most useful applications of AI in their shop for their productivity and their customers' experience.

> [!success] Line of Impact
> 
> These plans are in line with what I depicted and what I believe are the most important tasks I should work on as of today.

On a longer-term view I am creating a PhD opportunity with a focus on safety. I cannot say much for now but it should involve multi-agent systems safety through interpretability. Meanwhile I did refuse an industry PhD as I was worried I would be too constrain to explore the topics I wanted. This watershed opportunity might also be the occasion for me to start a podcast as a logical extension of this blog.

Regarding my involvement in the AIS sphere I am planning to be a mentor on future iteration of AIS programs. I keep an eye on Cherry, AIS Camps, SPAR should they support the technical interpretability agenda.
