---
title: My Approach to AI Safety
tldr: I stand in the line of research of interpretability as tool for monitoring and control. I am convinced that this the best path towards making AI safe in the long term but also in the short term. Shortcoming issues should not be undermined because if not tackle immediately they will make alignment a lot more complex.
tags:
  - AIS
  - Agenda
  - Startup
references: 
aliases: 
crossposts: 
publishedOn: 2023-11-02
editedOn: 2023-12-14
authors:
  - "[[Yoann Poupart]]"
---
> [!tldr] TL;DR
> 
> I stand in the line of research of interpretability as tool for monitoring and control. I am convinced that this the best path towards making AI safe in the long term but also in the short term. Shortcoming issues should not be undermined because if not tackle immediately they will make alignment a lot more complex.

## My Claims

- AI misuse is and will remain more concerning than AI alignment
- Widespread modelsâ€™ internal research, like mechanistic interpretability, is very likely to accelerate capabilities
- AIS entrepreneurship has more potential and fewer flaws than open AIS research
- Accelerating my career is positive for Alignment

## AI Misuse

Like steering is a double edge sword. You could either make your model steer towards love or steer towards hate. It feels more like weaponization rather than alignment. But let's say it works so well that alignment is solved, how do you prevent misuse of this new method?

I acknowledge my point is a bit far fetched yet it raises questions about whether Alignment is the ultimate problem. It dosen't make the alignment problem less important, but rather with less priority.

## My Agenda

## Founding an AI startup

## Possibilism is Unsustainable

## Overall assessment